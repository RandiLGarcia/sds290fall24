---
title: "Four Desgins"
author: Prof Randi Garcia, SDS 290
date: October 2, 2024
format: 
  revealjs:
    scrollable: true
    theme: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE)
```

```{r, include = FALSE}
library(Stat2Data)
library(tidyverse)
data("Diamonds")
ds <- Diamonds %>%
  group_by(Color) %>%
  summarise(n = n(),
            m = mean(TotalPrice),
            sd = sd(TotalPrice))
```

## Announcements

-   Working on HW3 grades
-   HW4 due on Friday 
-   Office hours (Bass 412)
    -   Friday office hours cancelled
    -   By appointment
-   Where to get HW help
    -   [Spinelli center](https://www.smith.edu/qlc/tutoring.html) tutoring Sun-Thurs 7-9p, Sabin-Reed 301. Nora Z, Cindy, and Sarah can help with 290.
    -   Post questions to [hw4-questions](https://smith.enterprise.slack.com/archives/C07PFFF2H62) Slack channel. 

## Agenda

-   Warm up
-   Four designs
-   MP1 time

## Warm-up: Walking Babies Example

As a rule it takes about a year before a baby takes its first steps alone. Scientists wondered if they could get babies to walk sooner by prescribing a set of special exercises. They decided to compare babies given special exercises with a control group of babies. But the scientists recognized that just showing an interest in the babies and their parents could cause a placebo effect. That is, the attention alone could influence parents and their babies in ways that would shorten the time to walk. 24 babies were randomly assigned to one of four conditions:

1.  **Special exercises**: Shown special exercises and parents are called weekly and asked it their baby was walking.
2.  **Exercise control**: No special exercises but parents were told to have baby exercise for 15 min every day. Parents were called each week.
3.  **Weekly report**: No exercises but parents were called each week.
4.  **Final report**: Only a report at the end on the study.

They recorded the age (in months) the babies first walked.

## Warm-up: Walking Babies Example

```{r, eval=FALSE}
data("WalkingBabies")
qplot(x = Group, y = Age, data = WalkingBabies, geom = "boxplot")
```

Conduct an ANOVA to test if there are mean differences in walking age between treatments. Check the ANOVA conditions (SINZ only). What do you conclude?

![](https://i.gifer.com/ZWGT.gif)

## Warm-up: Walking Babies Example

```{r}
data("WalkingBabies")
qplot(x = Group, y = Age, data = WalkingBabies, geom = "boxplot")
```

## Warm-up: Walking Babies Example

```{r}
WalkingBabies %>%
  group_by(Group) %>%
  summarise(n = n(),
            m = mean(Age),
            sd = sd(Age))
1.90/0.871
```

S: No...but could be worse. I: babies are in different families.

## Warm-up: Walking Babies Example

```{r}
babyMod <- lm(Age ~ Group, data = WalkingBabies)
anova(babyMod)
```

## Warm-up: Walking Babies Example

```{r}
plot(babyMod, which = 2)
```

N: looks ok with possible skew

## Warm-up: Walking Babies Example

```{r}
qplot(x = babyMod$residuals, bins = 8)
```

N: normality is satisfied; Z: Yes, centered at zero.

<!-- ## Adjusting for Multiple Comparisons -->

<!-- When we do multiple significance tests (or construct multiple confidence intervals), our effective type I error rate is inflated. Most statisticians agree that we should adjust our type I error rate to account for our multiple tests, and control the **familywise** error rate. -->

<!-- Four methods for controlling the familywise : -->

<!-- 1.  **Fisher Least Significant Difference (Fisher's LSD)** -->
<!-- 2.  Tukey Honest Significant Difference (Tukey's HSD) -->
<!-- 3.  Scheffe test -->
<!-- 4.  **The Bonferroni correction** -->

<!-- ## Fisher's LSD -->

<!-- Fisher's LSD reasons that a pairwise difference is significant as long as it's larger than the margin of error for that pairwise comparison. Thee step process: -->

<!-- -   **Step 1**: Is the omnibus ANOVA F-test significant? No $\longrightarrow$ Stop, Yes $\longrightarrow$ Step 2, -->

<!--   -->

<!-- -   **Step 2**: Find the least significant difference (LSD) for each pair: -->

<!-- $$LSD = t^*\cdot SD \sqrt{1/n_1+1/n_2}$$ -->

<!-- -   Where $t^*$ is your critical $t$ for the $df_{error}$ and your level of confidence, -->
<!-- -   and $SD = \sqrt{MSE}$. -->

<!--   -->

<!-- -   **Step 3**: Declare each difference of group means significant if the difference is as large as the LSD. -->

<!-- ## Fisher's LSD -->

<!-- For the sandwich ants. -->

<!-- ```{r, include=FALSE} -->
<!-- data("SandwichAnts") -->

<!-- SandwichAnts %>% -->
<!--   group_by(Filling) %>% -->
<!--   summarise(n = n(), -->
<!--             m = mean(Ants), -->
<!--             sd = sd(Ants)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- mean_v = 34.625 -->
<!-- mean_h = 55.500 -->
<!-- mean_p = 40.375 -->

<!-- MSE = 157.01 #from our ANOVA source table -->
<!-- df_E = 45 #from our ANOVA source table -->
<!-- t <- qt(.975, df_E) #for 95% CI -->

<!-- n = 16 #this is a balance design! -->
<!-- LSD = t*sqrt(MSE)*sqrt(1/n+1/n) -->
<!-- LSD -->
<!-- ``` -->

<!-- ## Fisher's LSD -->

<!-- For the sandwich ants. Fisher's **LSD is 8.923**. -->

<!-- ```{r} -->
<!-- mean_h-mean_p -->
<!-- mean_h-mean_v -->
<!-- mean_p-mean_v -->
<!-- ``` -->

<!-- Based on Fisher's LSD there are statistically significant differences between the number of ants on the ham and pickles filled sandwiches and both the peanut butter and Vegemite sandwiches, but not between the peanut butter and Vegemite sandwiches. Do ants like meat or pickles?! -->

<!-- ## Bonferroni Correction -->

<!-- The bonferroni correction simply takes our type I error rate ($\alpha = .05$) and divides by the number of tests we're conducting. For sandwich ants it is 3. Here's the n choose k formula again for sandwich ants: -->

<!-- $${3 \choose 2} = \frac{3!}{2!(3-2)!}=3$$ -->

<!-- ```{r} -->
<!-- #bonferroni correction -->
<!-- .05/3 -->
<!-- ``` -->

<!-- We could conduct $t$-tests (or multiple regression!) for all pairs of conditions, but use an alpha of .0167 (instead of .05) to make your decision about rejecting the null hypothesis. Your $p$-value will need to be less than .0167 to reject the null hypothesis. -->

<!-- ## Using Multiple Regression -->

<!-- ```{r} -->
<!-- antsMod <- lm(Ants ~ Filling, data = SandwichAnts) -->
<!-- summary(antsMod) -->
<!-- ``` -->

<!-- ## Using Multiple Regression -->

<!-- ```{r, echo=FALSE} -->
<!-- antsMod <- lm(Ants ~ Filling, data = SandwichAnts) -->
<!-- summary(antsMod) -->
<!-- ``` -->

<!-- Using multiple regression with a bonferroni correction applied, what do we conclude about the differences in ants between filling conditions? Which comparison is missing? -->

## Kelly's Hamster Study

![](02_four_designs-figure/pumpkin2.png)

## Kelly's Hamster Study

![](02_four_designs-figure/SP_RM_data.png)

## Design Principal 1: Random Assignment

Design 1: One-Way Randomized Design

![](02_four_designs-figure/BF1.png)

## Design Principal 2: Blocking

Design 2: One-Way Complete Block Design

![](02_four_designs-figure/CB1.png)

## Design Principal 3: Factorial Crossing

Design 3: Two-Way Factorial Design

![](02_four_designs-figure/BF2.png)

## Kelly's Hamster Study

![](02_four_designs-figure/SP_RM_data.png)

## Blocking + Random Assignment + Crossing

Design 4: Split Plot/Repeated Measures Design

![](02_four_designs-figure/SP_RM_data.png)

## Blocking + Random Assignment + Crossing

Design 4: Split Plot/Repeated Measures Design

![](02_four_designs-figure/SP_RM_small.png)

## Time to work on MP1

-   Log in to [Qualtrics](https://www.smith.edu/your-campus/offices-services/institutional-research/surveys) and enter your Smith credentials.
    -   I usually just Google "Smith Qualtrics"
