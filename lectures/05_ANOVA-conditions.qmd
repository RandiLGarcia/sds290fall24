---
title: "ANOVA Conditions"
author: Prof Randi Garcia
date: September 18, 2024
format: 
  revealjs:
    scrollable: true
    theme: default
---

## Announcements

-   Homework 2 due Friday 11:59p
    -   [Instructions for submitting pdf on gradescope](https://www.youtube.com/watch?v=nksyA0s-Geo)
-   Office hours (Bass 412)
    -   Friday 3:00-4:00p
-   Where to get HW help
-   Where to get HW help
    -   [Spinelli center](https://www.smith.edu/qlc/tutoring.html) tutoring Sun-Thurs 7-9p, Sabin-Reed 301. Nora Z (Sun, Tues), Cindy (Tue, Thurs), and Sarah (Tue) can help with 290!

## Review of last class

-   Introduced the motivation behind the Analysis of Variance (ANOVA)
-   Data decomposition
-   Assembly line or CAT scan metaphor

Assembly line for animal data: 

*Calmness* = *Grand Mean* + *Animal Effect* + *Cue Effect* + *Interaction* + *Student Effect* + *Residuals*

-   [Animal Data](https://randilgarcia.github.io/sds290fall24/code/01_animal_data.qmd)
-   [Assembly Line Metaphor code](https://randilgarcia.github.io/sds290fall24/code/02_informal_anova.qmd)

## Leafhoppers

Bar graph of treatment condition averages.



## Leafhoppers

```{r, echo=FALSE}
library(knitr)
library(tidyverse)
library(mosaic)

leaf <- data.frame(diet = c("control","control","sucrose", "sucrose", "glucose", "glucose", "fructose","fructose"), days = c(2.3,1.7,3.6,4.0,2.9,2.7,2.1,2.3))

leaf <- leaf %>%
  mutate(benchmark = mean(days)) %>%
  group_by(diet) %>%
  mutate(grp_mean = mean(days),
         diet_effect = grp_mean - benchmark) %>%
  ungroup() %>%
  mutate(fitted = benchmark + diet_effect,
         resid = days - grp_mean,
         resid_alt = days - fitted)
```

-   Now we start thinking about if those differences in treatment effects are real, or could possibly be due to chance error.

```{r, echo = FALSE}
kable(data.frame(' ' = c("","","means", "group effects"), control = c(2.3,1.7, 2.0, -0.7), sucrose = c(3.6, 4.0, 3.8, 1.1), glucose = c(2.9,2.7,2.8,0.1), fructose = c(2.1,2.3,2.2, -0.5)))
```

## One-way ANOVA Model

$$Y = f(X) + \epsilon$$ For t-test our model is: $$Y = \mu_i + \epsilon$$ For One-Way ANOVA our model is: $$Y = \mu + \alpha_i + \epsilon$$ where $\mu$ is the grand mean, $\alpha_i$ is the treatment effect (difference from the grand mean for the $i^{th}$ group), and $\epsilon$ is the residual.

<!-- ### Note about using SLR for categorical variables -->

<!-- When we have a binary explanatory variable, X, where X is an indicator of category 2: -->

<!-- $$Y = \beta_0+\beta_1X_{indicator}+\epsilon$$ where $\beta_0$ is the mean of category 1, and $\beta_1$ is the difference between the mean of category 1 and the mean of category 2. -->

## Analysis of Variance (ANOVA)

Formal ANOVA starts with the simple idea that we can compare our estimate of **treatment effect variability** to our estimate of **chance error variability** to measure how large our treatment effect is.  \
$$Variability\:in\:Treatment\:E\mathit{f}\mathit{f}ects = True\:E\mathit{f}\mathit{f}ect\:Di\mathit{f}\mathit{f}erences + Error$$ $$Variability\:in\:Residuals = Error$$  \
We can construct a comparison, which we will call F:  \
$$F = \frac{Variability\:in\:Treatment\:E\mathit{f}\mathit{f}ects}{Variability\:in\:Residuals}=\frac{True\:E\mathit{f}\mathit{f}ect\:Di\mathit{f}\mathit{f}erences + Error}{Error}$$  \
If our null hypothesis, ${H}_{0}: True\:E\mathit{f}\mathit{f}ect\:Di\mathit{f}\mathit{f}erences=0$, is true, then what would we expect the F-ratio to equal?

## Sum of Squares (SS)

ANOVA measures variability in treatment effects with the sum of squares ($SS$) divided by the number of units of unique information ($df$). For the One-Way design:

$${SS}_{Treatments} = n\sum_{i=1}^{a}(\bar{y}_{i.}-\bar{y}_{..})^{2}$$

$${SS}_{E} = \sum_{i=1}^{a}\sum_{j=1}^{n}({y}_{ij}-\bar{y}_{i.})^{2}$$

$${SS}_{Total} = {SS}_{Treatments} + {SS}_{E}$$

where $n$ is the group size, and $a$ is the number of treatments.

## Degrees of Freedom (df)

The $df$ for a table equals the number of free numbers, the number of slots in the table you can fill in before the pattern of repetitions and adding to zero tell you what the remaining numbers have to be.

$${df}_{Treatments}=a-1$$

$${df}_{E}=N-a$$

## Mean Squares (MS)

The ultimate statistic we want to calculate is $F = \frac{Variability\:in\:Treatment\:E\mathit{f}\mathit{f}ects}{Variability\:in\:Residuals}$.

**Variability in treatment effects**: $${MS}_{Treatments}=\frac{{SS}_{Treatments}}{{df}_{Treatments}}$$

**Variability in residuals** $${MS}_{E}=\frac{{SS}_{E}}{{df}_{E}}$$

## F-ratios and the F-distribution

Finally, the ratio of these two MS's is called the F ratio. The following quantity is our test statistic for the null hypothesis that there are no treatment effects.

$$F = \frac{{MS}_{Treatments}}{{MS}_{E}}$$

If the null hypothesis is true, then F is a random variable $\sim F({df}_{Treatments}, {df}_{E})$. The [F-distribution](https://en.wikipedia.org/wiki/F-distribution).

```{r, echo = TRUE}
qplot(x = rf(500, 3, 4), geom = "density")
```

## Inference Testing in ANOVA

$${H}_{0}:\alpha_1=\alpha_2=\alpha_3=\alpha_4=0$$

*OR*, ${H}_{0}:\mu_1=\mu_2=\mu_3=\mu_4$

$$H_a: Some\:\alpha_i\neq 0$$

Or ${H}_{A}:some\:\mu_i\neq\mu_j$. We can find the p-value for our F calculation with the following code

```{r, echo = TRUE}

pf(17.422, 3, 4, lower.tail = FALSE)
```

## ANOVA Source Table Leafhoppers

```{r, echo = TRUE}
mod <- lm(days ~ diet, data = leaf)
anova(mod)
```

## ANOVA for One-Way ANOVA

Model:

$${y}_{ij}=\mu+{\alpha}_{i}+{e}_{ij}$$

Null hypothesis:

$${H}_{0}:{\alpha}_{1}={\alpha}_{2}=...={\alpha}_{a}=0$$

ANOVA Source table:

|    Source |                            SS                             |  df   |                      MS                       |                  F                   |
|--------------:|:-------------:|:-------------:|:-------------:|:-------------:|
| Treatment |     $n\sum_{i=1}^{a}(\bar{y}_{i.}-\bar{y}_{..})^{2}$      | $a-1$ | $\frac{{SS}_{Treatments}}{{df}_{Treatments}}$ | $\frac{{MS}_{Treatments}}{{MS}_{E}}$ |
|     Error | $\sum_{i=1}^{a}\sum_{j=1}^{n}({y}_{ij}-\bar{y}_{i.})^{2}$ | $N-a$ |          $\frac{{SS}_{E}}{{df}_{E}}$          |                                      |

## Running an ANOVA in R

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
leaf <- data.frame(diet = c("control","control","sucrose", "sucrose", "glucose", "glucose", "fructose","fructose"), days = c(2.3,1.7,3.6,4.0,2.9,2.7,2.1,2.3))
```

Three steps:

1.  Boxplots or some data visualization
2.  Descriptive statistics (n, means, standard deviations)
3.  ANOVA
4.  Check assumptions

## Step 1. Boxplots

```{r, echo = TRUE}
#Note that boxplots are a bit silly in this example because n is only 2.
qplot(x = diet, y = days, data = leaf, geom = "boxplot")
```

## Step 2. Descriptive Statistics

```{r, echo = TRUE}
leaf %>%
  group_by(diet) %>%
  summarize(n = n(),
            m = mean(days),
            sd = sd(days))
```

## Step 3. ANOVA

```{r, echo = TRUE}
mod <- lm(days ~ diet, data = leaf)
anova(mod)
```

### Write a sentence interpreting the One-Way ANOVA results

There are statistically significant differences in leafhopper survival rates across diets, $F(3, 4) = 17.42$, $p = .009$.

**OR**

Diet has a statistically significant effect on leafhopper survival rates, $F(3, 4) = 17.42$, $p = .009$.

## Try it!

In groups of three run all three steps (visualize, descriptive stats, and ANOVA) for the two datasets below. Write a sentence about your conclusion from the ANOVA.

**Dataset 1**: `SandwichAnts`

-   Factor: `Bread`
-   Response: `Ants`

**Dataset 2**: `Meniscus`

-   Factor: `Method`
-   Response: `Displacement`

```{r, echo = TRUE, eval=FALSE}
library(Stat2Data)
data("SandwichAnts")
?SandwichAnts
```

## ANOVA Conditions

**STOP!!**...only under certain conditions can we rely on this inference. There were many assumptions built into:

-   how we decomposed the data, and
-   our thoughts about residual errors.

### Simulation in R: Assembly Line Metaphor

-   Recall the [Assembly Line Metaphor and code](https://randilgarcia.github.io/sds290fall24/code/02_informal_anova.qmd)

## ANOVA Conditions

![](pics/CA-SINZ2.png)

-   **C**. Constant effects
-   **A**. Additive effects
-   **S**. Same standard deviations
-   **I**. Independent residuals
-   **N**. Normally distributed residuals
-   **Z**. Zero mean residuals

## C. Constant effects

We assume every observation in a similar condition is affected exactly the same. (Gets the same "true score").

```{r, eval=FALSE}
animals_sim <- animals %>%
  mutate(benchmark = mean(calm)) %>%
  group_by(animal) %>%
  mutate(animal_mean = mean(calm),
         aminal_effect = animal_mean - benchmark)
```

## A. Additive effects

We add the effects as we go down the assembly line.

The interaction effect captures the possibility that conditions have non-additive effects, but it is also added to everything else.

```{r, echo = TRUE, eval=FALSE}
calm_sim = benchmark
         + aminal_effect
         + cue_effect
         + interaction_effect
         + student_effect
```

## S. Same standard deviations

The piece of code for adding error is not dependent on which condition the observations is in.

```{r, echo = TRUE, eval=FALSE}
 + rnorm(80, 0, 1.16)
```

## I. Independent residuals

Takes 80 independent draws from a normal distribution.

```{r, echo = TRUE, eval=FALSE}
 + rnorm(80, 0, 1.16)
```

## N. Normally distributed residuals

It's `rnorm()`, and not `rbinom()` or `rpois()`...

```{r, echo = TRUE, eval=FALSE}
 + rnorm(80, 0, 1.16)
```

## Z. Zero mean residuals

The second argument is the mean.

```{r, echo = TRUE, eval=FALSE}
 + rnorm(80, 0, 1.16)
```

## How to check assumptions

-   C. **Constant effects** -- *think* about whether it is reasonable.

-   A. **Additive effects** -- *think* about whether it is reasonable.

-   S. **Same standard deviations** -- is the biggest SD less than two times as large as the smallest?

-   I. **Independent residuals** -- *think* about whether it is reasonable.

-   N. **Normally distributed residuals** -- construct a histogram or normal probability plot of residuals.

-   Z. **Zero mean residuals** -- construct a histogram or normal probability plot of residuals.

## To check the Residual Assumptions: SINZ

**S**: calculate descriptive statistics and divide largest SD by smallest.

```{r, echo = TRUE}
leaf %>%
  group_by(diet) %>%
  summarize(n = n(),
            m = mean(days),
            sd = sd(days))

0.424/0.141
```

**I**: Judge for yourself -- what do we think about the leafhopper dishes?

## To check the Residual Assumptions: SINZ

**N**: Look at histogram and normal probability plots of residuals.

```{r, echo = TRUE}
#normal probability plot of residuals
plot(mod, which = 2)
```

## To check the Residual Assumptions: SINZ

**Z**: Look at histogram of residuals.

```{r, echo = TRUE}
#histogram of residuals
qplot(x = mod$residuals, bins = 4)
```
