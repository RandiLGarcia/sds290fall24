---
title: "Multiple Comparisons"
author: Prof Randi Garcia, SDS 290
date: September 25, 2024
format: 
  revealjs:
    scrollable: true
    theme: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = TRUE)
```

```{r, include = FALSE}
library(Stat2Data)
library(tidyverse)
data("Diamonds")
ds <- Diamonds %>%
  group_by(Color) %>%
  summarise(n = n(),
            m = mean(TotalPrice),
            sd = sd(TotalPrice))
```

## Announcements

-   HW3 due on Friday 
-   Office hours (Bass 412)
    -   Friday 3:00-4:00p
-   Where to get HW help
    -   [Spinelli center](https://www.smith.edu/qlc/tutoring.html) tutoring Sun-Thurs 7-9p, Sabin-Reed 301. Nora Z, Cindy, and Sarah can help with 290.
    -   Post questions to [hw3-questions](https://smith.enterprise.slack.com/archives/C07NL5E7K8A) Slack channel. 

## Agenda

-   Transformations
-   CI and Effect sizes
-   MP1 topics time
    -   Introduction to Qualtrics (video on Moodle)

## Finish up slides from Monday

-   [Data Transformations](https://randilgarcia.github.io/sds290fall24/lectures/06_CI-effect-sizes.html#/data-transformations)

## Diamond Price by Color

```{r}
ggplot(Diamonds, aes(x = Color, y = log(TotalPrice))) +
  geom_boxplot()
```

## ANOVA for Diamonds

```{r}
modDia <- lm(log(TotalPrice) ~ Color, data = Diamonds)
anova(modDia)
```

There are statistically significant differences in log price of diamonds across colors $F(6, 344) = 7.37$, $p < .001$.

But what else do we want to know??

## Effect Sizes and Confidence Intervals

-   The p-value for our F-ratio only tells us if effects are **detectable**.
    -   Answers the question: **IS there an effect?**
-   Practical significance is different from **statistical** significance.
    -   Need to answer the question: **How big is IT**?
-   Which values in ANOVA do we want to construct confidence intervals for (or calculate an effect size for)?

### Some options:

-   A specific group mean.
-   How far away a specific condition is from the grand average (the treatment effect).
-   **Pairwise group differences:** How far away a specific condition is from another condition.
-   How far away a specific condition (or set of conditions) is(are) from a(nother) set of conditions.
-   Additional effect size option: The overall variance explained by a factor of interest.

## CI for Pairwise Group Differences

$$(\bar{y_i}-\bar{y_j}) \pm t^*\cdot SD \sqrt{1/n_i+1/n_j}$$

Diamonds example: For difference between G (near colorless) and F (colorless)

```{r}
MSE = 0.6771 #from our ANOVA source table
df_E = 344 #from our ANOVA source table
t <- qt(.975, df_E) #for 95% CI

n_g = filter(ds, Color == "G")$n #sample size for G
n_f = filter(ds, Color == "F")$n #sample size for F

mean_g <- log(filter(ds, Color == "G")$m) #mean for G
mean_f <- log(filter(ds, Color == "F")$m) #mean for F

#Confidence interval
UL <- (mean_g-mean_f) + t*sqrt(MSE)*sqrt(1/n_g+1/n_f) #upper limit
LL <- (mean_g-mean_f) - t*sqrt(MSE)*sqrt(1/n_g+1/n_f) #lower limit
```

We are 95% confident that the true mean difference in (log) price between near colorless and colorless diamonds is between -0.20 and 0.29. (There is no evidence of a difference between mean prices for colorless and near colorless diamonds.)

## Effect Size

### Effect Size (D)

$$D_{ij} = \frac{(\bar{y_i}-\bar{y_j})}{SD}$$

-   Where again $SD = \sqrt{MSE}$

```{r}
D = (mean_g-mean_f)/sqrt(MSE)
D
```

The difference between the price of nearly colorless diamonds and colorless diamonds is only 0.052 times the size of the typical within-group deviations in price. Such a small difference!

## Effect Size

### R-squared for One-Way ANOVA

$R^2$ is the proportion of total variation explained by the factor:

$$R^2 = \frac{SS_{treatment}}{SS_{Total}}$$

```{r}
modDia <- lm(log(TotalPrice) ~ Color, data = Diamonds)
anova(modDia)
```

## Effect Size

### R-squared for One-Way ANOVA

```{r}
R2 = 29.946/(29.946+232.913)
R2
```

The $R^2$ value is 0.11, so about 11% of the variation in the total price of a diamonds is explained by the diamonds' colors.

## Try it on the Sandwich Ants data

Use the `SandwichAnts` dataset to:

1.  Test if there are difference in Ants depending on the sandwich `Filling` using **ANOVA**. Check the ANOVA conditions (**SINZ**) and interpret the results of your analysis in context. 
2.  Calculate the **CI** and **effect size (D)** for the difference between `HamPickles` and `PeanutButter`, and for the difference between `PeanutButter` and `Vegemite`. Interpret the results of your analysis in context.
3.  Calculate the $R^2$ for your model and interpret it in a sentence.

### I'll do it with you!

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRjFZHkSMoVLVMmPdJMbOWRxUEwMpDAi9kHiA&usqp=CAU)

<!-- ## Multiple Comparisons -->

<!-- If we want to do all pairwise comparisons for the walking babies, how many comparisons would we need to make? -->

<!-- ## Multiple Comparisons -->

<!-- If we want to do all pairwise comparisons for the leafhoppers, how many comparisons would we need to make? We could use the *n choose k* formula: -->

<!-- $${n \choose k} = \frac{n!}{k!(n-k)!}$$ -->

<!-- ## Multiple Comparisons -->

<!-- If we want to do all pairwise comparisons for the leafhoppers, how many comparisons would we need to make? We could use the *4 choose 2* formula: -->

<!-- $${4 \choose 2} = \frac{4!}{2!(4-2)!}=\frac{4\cdot3\cdot2\cdot1}{2\cdot1(2\cdot1)}=\frac{24}{4}=6$$ -->

<!-- ## Multiple Comparisons -->

<!-- If we want to do all pairwise comparisons for the leafhoppers, how many comparisons would we need to make? -->

<!-- $${n \choose k} = \frac{n!}{k!(n-k)!}$$ -->

<!-- -   When we have more than two levels of a factor of interest, we might want to compare specific groups to see which one differ from each other. We can -->
<!--     -   Compute a confidence interval for the difference between two groups. -->
<!--     -   Run a set of all **pairwise comparisons**. -->
<!--     -   Create **custom contrasts** of more complex ideas. -->

<!-- ### Recall: our Type I error rate is 5% for one test! If the null hypothesis were true, 1 out of 20 tests we run would be significant. -->

<!-- ## Adjusting for Multiple Comparisons -->

<!-- When we do multiple significance tests (or construct multiple confidence intervals), our effective type I error rate is inflated. Most statisticians agree that we should adjust our type I error rate to account for our multiple tests, and control the **familywise** error rate. -->

<!-- Four methods for controlling the familywise : -->

<!-- 1.  Fisher Least Significant Difference (Fisher's LSD) -->
<!-- 2.  Tukey Honest Significant Difference (Tukey's HSD) -->
<!-- 3.  Scheffe test -->
<!-- 4.  The Bonferroni correction -->

<!-- ## Adjusting for Multiple Comparisons -->

<!-- When we do multiple significance tests (or construct multiple confidence intervals), our effective type I error rate is inflated. Most statisticians agree that we should adjust our type I error rate to account for our multiple tests, and control the **familywise** error rate. -->

<!-- Four methods for controlling the familywise : -->

<!-- 1.  **Fisher Least Significant Difference (Fisher's LSD)** -->
<!-- 2.  Tukey Honest Significant Difference (Tukey's HSD) -->
<!-- 3.  Scheffe test -->
<!-- 4.  **The Bonferroni correction** -->

<!-- ## Fisher's LSD -->

<!-- Fisher's LSD reasons that a pairwise difference is significant as long as it's larger than the margin of error for that pairwise comparison. Thee step process: -->

<!-- -   **Step 1**: Is the omnibus ANOVA F-test significant? No $\longrightarrow$ Stop, Yes $\longrightarrow$ Step 2, -->

<!--   -->

<!-- -   **Step 2**: Find the least significant difference (LSD) for each pair: -->

<!-- $$LSD = t^*\cdot SD \sqrt{1/n_1+1/n_2}$$ -->

<!-- -   Where $t^*$ is your critical $t$ for the $df_{error}$ and your level of confidence, -->
<!-- -   and $SD = \sqrt{MSE}$. -->

<!--   -->

<!-- -   **Step 3**: Declare each difference of group means significant if the difference is as large as the LSD. -->

<!-- ## Fisher's LSD -->

<!-- For the sandwich ants. -->

<!-- ```{r, include=FALSE} -->
<!-- data("SandwichAnts") -->

<!-- SandwichAnts %>% -->
<!--   group_by(Filling) %>% -->
<!--   summarise(n = n(), -->
<!--             m = mean(Ants), -->
<!--             sd = sd(Ants)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- mean_v = 34.625 -->
<!-- mean_h = 55.500 -->
<!-- mean_p = 40.375 -->

<!-- MSE = 157.01 #from our ANOVA source table -->
<!-- df_E = 45 #from our ANOVA source table -->
<!-- t <- qt(.975, df_E) #for 95% CI -->

<!-- n = 16 #this is a balance design! -->
<!-- LSD = t*sqrt(MSE)*sqrt(1/n+1/n) -->
<!-- LSD -->
<!-- ``` -->

<!-- ## Fisher's LSD -->

<!-- For the sandwich ants. Fisher's **LSD is 8.923**. -->

<!-- ```{r} -->
<!-- mean_h-mean_p -->
<!-- mean_h-mean_v -->
<!-- mean_p-mean_v -->
<!-- ``` -->

<!-- Based on Fisher's LSD there are statistically significant differences between the number of ants on the ham and pickles filled sandwiches and both the peanut butter and Vegemite sandwiches, but not between the peanut butter and Vegemite sandwiches. Do ants like meat or pickles?! -->

<!-- ## Bonferroni Correction -->

<!-- The bonferroni correction simply takes our type I error rate ($\alpha = .05$) and divides by the number of tests we're conducting. For sandwich ants it is 3. Here's the n choose k formula again for sandwich ants: -->

<!-- $${3 \choose 2} = \frac{3!}{2!(3-2)!}=3$$ -->

<!-- ```{r} -->
<!-- #bonferroni correction -->
<!-- .05/3 -->
<!-- ``` -->

<!-- We could conduct $t$-tests (or multiple regression!) for all pairs of conditions, but use an alpha of .0167 (instead of .05) to make your decision about rejecting the null hypothesis. Your $p$-value will need to be less than .0167 to reject the null hypothesis. -->

<!-- ## Using Multiple Regression -->

<!-- ```{r} -->
<!-- antsMod <- lm(Ants ~ Filling, data = SandwichAnts) -->
<!-- summary(antsMod) -->
<!-- ``` -->

<!-- ## Using Multiple Regression -->

<!-- ```{r, echo=FALSE} -->
<!-- antsMod <- lm(Ants ~ Filling, data = SandwichAnts) -->
<!-- summary(antsMod) -->
<!-- ``` -->

<!-- Using multiple regression with a bonferroni correction applied, what do we conclude about the differences in ants between filling conditions? Which comparison is missing? -->

## Introduction to Qualtrics

-   Log in to [Qualtrics](smithcollege.qualtrics.com) and enter your Smith credentials.
    -   I usually just google "Smith Qualtrics"
    
## Mini Project 1

Topic brainstorming

1.  Two minute free write to write down some ideas for your factor of interest and response variable.
2.  Get into groups of three and take turns discussing your ideas.
3.  Raise your hand if you have any questions.    
